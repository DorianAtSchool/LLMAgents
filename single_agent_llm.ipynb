{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moutlines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, generate\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mtransformers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/opt-iml-max-1.3b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#regex of array of numbers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m generator \u001b[38;5;241m=\u001b[39m generate\u001b[38;5;241m.\u001b[39mregex(\n\u001b[1;32m      7\u001b[0m     model,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/outlines/models/transformers.py:430\u001b[0m, in \u001b[0;36mtransformers\u001b[0;34m(model_name, device, model_kwargs, tokenizer_kwargs, model_class, tokenizer_class)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m--> 430\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m    432\u001b[0m tokenizer_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding_side\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    433\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4254\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4257\u001b[0m         (\n\u001b[1;32m   4258\u001b[0m             model,\n\u001b[1;32m   4259\u001b[0m             missing_keys,\n\u001b[1;32m   4260\u001b[0m             unexpected_keys,\n\u001b[1;32m   4261\u001b[0m             mismatched_keys,\n\u001b[1;32m   4262\u001b[0m             offload_index,\n\u001b[1;32m   4263\u001b[0m             error_msgs,\n\u001b[0;32m-> 4264\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   4265\u001b[0m             model,\n\u001b[1;32m   4266\u001b[0m             state_dict,\n\u001b[1;32m   4267\u001b[0m             loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   4268\u001b[0m             resolved_archive_file,\n\u001b[1;32m   4269\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   4270\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   4271\u001b[0m             sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   4272\u001b[0m             _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   4273\u001b[0m             low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   4274\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4275\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4276\u001b[0m             offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   4277\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4278\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4279\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4280\u001b[0m             gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[1;32m   4281\u001b[0m             weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   4282\u001b[0m         )\n\u001b[1;32m   4284\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4285\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4712\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4707\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4708\u001b[0m         \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   4709\u001b[0m         assign_to_params_buffers \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(\n\u001b[1;32m   4710\u001b[0m             model_to_load, state_dict, start_prefix\n\u001b[1;32m   4711\u001b[0m         )\n\u001b[0;32m-> 4712\u001b[0m         error_msgs \u001b[38;5;241m=\u001b[39m _load_state_dict_into_model(\n\u001b[1;32m   4713\u001b[0m             model_to_load, state_dict, start_prefix, assign_to_params_buffers\n\u001b[1;32m   4714\u001b[0m         )\n\u001b[1;32m   4716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4717\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n\u001b[1;32m   4718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:724\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n\u001b[0;32m--> 724\u001b[0m load(model_to_load, state_dict, prefix\u001b[38;5;241m=\u001b[39mstart_prefix, assign_to_params_buffers\u001b[38;5;241m=\u001b[39massign_to_params_buffers)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:722\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:722\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 722 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:722\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:718\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2441\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2440\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2441\u001b[0m             param\u001b[38;5;241m.\u001b[39mcopy_(input_param)\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2443\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from outlines import models, generate\n",
    "\n",
    "model = models.transformers(\"facebook/opt-iml-max-1.3b\")\n",
    "\n",
    "generator = generate.regex(\n",
    "    model,\n",
    "    r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s>result of 9 + 9 = 18</s><s>result of 1 + 2 = \"\n",
    "answer = generate.format(model, int)(prompt)\n",
    "print(answer)\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.1.1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the IP address of the Google DNS servers? \"\n",
    "answer = generator(prompt, max_tokens=30)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cost_table(prompt):\n",
    "    # Load the model\n",
    "    # load falcon model\n",
    "    llm = models.transformers(\"facebook/opt-iml-max-1.3b\")\n",
    "    generator = generate.regex(\n",
    "        model,\n",
    "        r\"\\d+\"\n",
    "    )\n",
    "    # llm = models.Transformers(\"google/flan-t5-large\n",
    "\n",
    "    print(\"generator loaded\")\n",
    "    def cost_table_generator(lm):\n",
    "        # Define the full context with instructions and examples\n",
    "        context = f\"\"\"\n",
    "          Convert the new description to a Python list.\n",
    "            Format: [cost at hour 12:00AM-1:00AM, cost at hour 1:01AM-2:00AM, ..., cost at hour 10:01PM-11:00PM, cost at hour 11:01PMM-11:59PM]\n",
    "            Rules:\n",
    "            - Only return list of numbers\n",
    "            - List must have 24 items\n",
    "            - Match input constraints exactly\n",
    "\n",
    "           \n",
    "            New Description:\n",
    "            [{prompt}] ->\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the cost table with regex constraint\n",
    "       \n",
    "\n",
    "        result = generator(context, max_tokens=100)\n",
    "        return result\n",
    "\n",
    "    # Run the generator\n",
    "    result = cost_table_generator(llm)\n",
    "     # Safely extract and evaluate the cost table\n",
    "    print(\"result: \", result)\n",
    "    # cost_table_str = result.text().split('-> [')[1].strip()[:-1]  # Remove the trailing ]\n",
    "    # print\n",
    "    # Extract and return the cost table\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class for cost table, initialize with model and generator, add fucntion to generate cost table\n",
    "import os\n",
    "def cost_table_generator(model, regex):\n",
    "    class CostTableGenerator:\n",
    "        def __init__(self, model, regex):\n",
    "            self.llm = models.transformers(model)\n",
    "            self.generator = generate.regex(\n",
    "                self.llm,\n",
    "                regex\n",
    "            )\n",
    "            print(\"generator loaded\")\n",
    "            return\n",
    "        \n",
    "        def generate_cost_table(self, prompt):\n",
    "            context = f\"\"\"\n",
    "            Convert the new description to a Python list.\n",
    "                Format: [cost at hour 12:00AM-1:00AM, cost at hour 1:01AM-2:00AM, ..., cost at hour 10:01PM-11:00PM, cost at hour 11:01PMM-11:59PM]\n",
    "                Rules:\n",
    "                - Only return list of numbers\n",
    "                - List must have 24 items\n",
    "                - Match input constraints exactly\n",
    "\n",
    "            \n",
    "                New Description:\n",
    "                [{prompt}] ->\n",
    "                    \n",
    "            \"\"\"\n",
    "            # Try with 2 \"training\" and 1 \"test\"\n",
    "        #   - Certain times are completely unavailable (e.g., sleep, existing meetings).\n",
    "        #   - Some times are less ideal (e.g., early morning or late evening).\n",
    "        #   - Certain times are optimal for scheduling activities (e.g., typical work hours or daylight hours).\n",
    "\n",
    "        # You will be given some examples, and then a new context prompt. Your task is to generate a cost table for the new context prompt. Only return the array of numbers.\n",
    "\n",
    "        # param: *context_prompt*: includes existing meetings, preferences, and constraints\n",
    "\n",
    "    # prompt = \"\"\"\n",
    "\n",
    "    # TASK DESCRIPTION:\n",
    "    #     Generate 24-hour cost table that aligns with natural language constraints and preferences for scheduling activities.\n",
    "\n",
    "    #        - Certain times are completely unavailable (e.g., sleep, existing meetings).\n",
    "    #        - Some times are less ideal (e.g., early morning or late evening).\n",
    "    #        - Certain times are optimal for scheduling activities (e.g., typical work hours or daylight hours).\n",
    "\n",
    "    #      You will be given some examples, and then a new context prompt. Your task is to generate a cost table for the new context prompt. Only return the 24-hours cost table array of numbers.\n",
    "\n",
    "    #     param: *context_prompt*: includes existing meetings, preferences, and constraints\n",
    "    #     Examples:\n",
    "\n",
    "    # \"context_prompt\":\n",
    "    # \"From 12:00 AM to 6:59 AM, scheduling is unreasonable due to a preference against early appointments, so this time has a very high cost of 1000. From 7:00 AM to 8:59 AM, the time is available but less ideal, warranting a moderate cost of 50. From 9:00 AM to 2:00 PM, work constraints render these times unavailable with an INFINITE cost. Between 2:00 PM and 2:59 PM, this period is flexible and ideal for the doctor's appointment, so it has a cost of 0. From 3:00 PM to 3:59 PM, the dentist appointment makes this hour unavailable with an INFINITE cost. From 4:00 PM to 9:00 PM, scheduling is optimal, also with a cost of 0. Finally, from 9:01 PM to 11:59 PM, late hours are less preferable and have a high cost of 500.\"\n",
    "    # \"goal\": \"Generate an array for a 24-hour cost table where each hour reflects its scheduling cost based on the constraints provided.\"\n",
    "    # ”cost_table”: [\n",
    "    #     1000, 1000, 1000, 1000, 1000, 1000, 1000,  # 12:00 AM to 6:59 AM\n",
    "    #     50, 50,  # 7:00 AM to 8:59 AM\n",
    "    #     float('inf'), float('inf'), float('inf'), float('inf'), float('inf'),  # 9:00 AM to 2:00 PM\n",
    "    #     0,  # 2:00 PM to 2:59 PM\n",
    "    #     float('inf'),  # 3:00 PM to 3:59 PM\n",
    "    #     0, 0, 0, 0, 0,  # 4:00 PM to 9:00 PM\n",
    "    #     500, 500, 500  # 9:01 PM to 11:59 PM\n",
    "    # ]\n",
    "\n",
    "    # \"context_prompt\":\n",
    "    # \"From 12:00 AM to 5:00 AM, scheduling soccer practice is unreasonable as it falls outside daylight hours, resulting in a high cost of 500. Between 5:01 AM and 7:00 AM, the early sunrise makes it less ideal, leading to a moderate cost of 100. From 7:01 AM to 4:00 PM, this time is available but not optimal, warranting a low-moderate cost of 50. The period from 4:00 PM to 5:59 PM is ideal for soccer practice, so the cost is 0. From 6:00 PM to 6:59 PM, the yoga class creates a scheduling conflict with an INFINITE cost. Finally, from 7:00 PM to 11:59 PM, scheduling falls outside preferred daylight hours, with a cost ranging linearly from 50 to 100.\"\n",
    "    # \"goal\": \"Generate an array for a 24-hour cost table where each hour reflects its scheduling cost based on the constraints provided.\"\n",
    "    # ”cost_table” : [\n",
    "    #     500, 500, 500, 500, 500,  # 12:00 AM to 5:00 AM\n",
    "    #     100, 100,  # 5:01 AM to 7:00 AM\n",
    "    #     50, 50, 50, 50, 50, 50, 50, 50, 50,  # 7:01 AM to 4:00 PM\n",
    "    #     0, 0,  # 4:00 PM to 5:59 PM\n",
    "    #     float('inf'),  # 6:00 PM to 6:59 PM\n",
    "    #     50, 62, 75, 87, 100  # 7:00 PM to 11:59 PM (linear increase)\n",
    "    # ]\n",
    "\n",
    "\n",
    "\n",
    "    # ]\n",
    "\n",
    "\n",
    "\n",
    "    # \"CONTEXT PROMPT\":\n",
    "    # \"From 12:00 AM to 7:59 AM, these hours are available but less ideal for a doctor's appointment, resulting in a high cost of 200. Between 8:00 AM and 11:59 AM, work constraints render scheduling impossible, with an INFINITE cost. From 12:00 PM to 12:59 PM, this hour is optimal for a doctor's appointment, with a cost of 0. Between 1:00 PM and 4:59 PM, work hours make scheduling unavailable, resulting in an INFINITE cost. From 5:00 PM to 8:59 PM, this period is flexible and available, so the cost is 0. Finally, from 9:00 PM to 11:59 PM, late hours are less ideal for a doctor's appointment, resulting in a high cost of 500.\"\n",
    "    # NEW GOAL: \"Generate an array for a 24-hour cost table where each hour reflects its scheduling cost based on the constraints provided.\"\n",
    "\n",
    "    # \"COST TABLE\" :[\n",
    "\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "\n",
    "            structured_prompt = f\"\"\"\n",
    "\n",
    "            Convert the following description to a Python list.\n",
    "            Format: [cost at hour 12:00AM-1:00AM, cost at hour 1:01AM-2:00AM, ..., cost at hour 10:01PM-11:00PM, cost at hour 11:01PMM-11:59PM]\n",
    "            Rules:\n",
    "            - Only return list of numbers\n",
    "            - List must have 24 items\n",
    "            - Match input constraints exactly\n",
    "\n",
    "\n",
    "            New Context Prompt:\n",
    "            [{prompt}] -> \n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"\\nGenerating new cost time table\")\n",
    "\n",
    "            print(\"Using openai: \\n\")\n",
    "            # use chatgpt key\n",
    "            from openai import OpenAI\n",
    "            os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "            client = OpenAI()\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{structured_prompt}\"\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(response.choices[0].message.content)\n",
    "            context = structured_prompt\n",
    "            result = self.generator(context, max_tokens=200)\n",
    "            print(\"llm result: \", result)\n",
    "            return result\n",
    "        \n",
    "        \n",
    "    \n",
    "    return CostTableGenerator(model, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
=======
   "execution_count": 10,
>>>>>>> parent of 0049021 (single agnet llm - outlines and pydantic attempt)
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom 12:00 AM to 7:59 AM, these hours are available but less ideal for a doctor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms appointment, resulting in a high cost of 200. Between 8:00 AM and 11:59 AM, work constraints render scheduling impossible, with an INFINITE cost. From 12:00 PM to 12:59 PM, this hour is optimal for a doctor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms appointment, with a cost of 0. Between 1:00 PM and 4:59 PM, work hours make scheduling unavailable, resulting in an INFINITE cost. From 5:00 PM to 8:59 PM, this period is flexible and available, so the cost is 0. Finally, from 9:00 PM to 11:59 PM, late hours are less ideal for a doctor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms appointment, resulting in a high cost of 500.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m gen \u001b[38;5;241m=\u001b[39m generate_cost_table(p)\n",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m, in \u001b[0;36mgenerate_cost_table\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run the generator\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m result \u001b[38;5;241m=\u001b[39m cost_table_generator(llm)\n\u001b[1;32m     36\u001b[0m  \u001b[38;5;66;03m# Safely extract and evaluate the cost table\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mgenerate_cost_table.<locals>.cost_table_generator\u001b[0;34m(lm)\u001b[0m\n\u001b[1;32m     14\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  Convert the new description to a Python list.\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    Format: [cost at hour 12:00AM-1:00AM, cost at hour 1:01AM-2:00AM, ..., cost at hour 10:01PM-11:00PM, cost at hour 11:01PMM-11:59PM]\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m        \u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate the cost table with regex constraint\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m result \u001b[38;5;241m=\u001b[39m generator(context, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/outlines/generate/api.py:504\u001b[0m, in \u001b[0;36mSequenceGeneratorAdapter.__call__\u001b[0;34m(self, prompts, max_tokens, stop_at, seed, **model_specific_params)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt of list of prompts.\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m generation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_generation_parameters(\n\u001b[1;32m    501\u001b[0m     max_tokens, stop_at, seed\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    505\u001b[0m     prompts,\n\u001b[1;32m    506\u001b[0m     generation_params,\n\u001b[1;32m    507\u001b[0m     copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits_processor),\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_params,\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_specific_params,\n\u001b[1;32m    510\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format(completions)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/outlines/models/transformers.py:247\u001b[0m, in \u001b[0;36mTransformers.generate\u001b[0;34m(self, prompts, generation_parameters, logits_processor, sampling_parameters)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    241\u001b[0m generation_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_kwargs(\n\u001b[1;32m    242\u001b[0m     prompts,\n\u001b[1;32m    243\u001b[0m     generation_parameters,\n\u001b[1;32m    244\u001b[0m     logits_processor,\n\u001b[1;32m    245\u001b[0m     sampling_parameters,\n\u001b[1;32m    246\u001b[0m )\n\u001b[0;32m--> 247\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_output_seq(prompts, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# if single str input and single sample per input, convert to a 1D output\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompts, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/outlines/models/transformers.py:350\u001b[0m, in \u001b[0;36mTransformers._generate_output_seq\u001b[0;34m(self, prompts, inputs, generation_config, **generation_kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_output_seq\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts, inputs, generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs\n\u001b[1;32m    348\u001b[0m ):\n\u001b[1;32m    349\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# encoder-decoder returns output_ids only, decoder-only returns full seq ids\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2253\u001b[0m         input_ids,\n\u001b[1;32m   2254\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2255\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2256\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2257\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2258\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:1176\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1177\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1178\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1179\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1180\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1181\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1182\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1183\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1184\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1185\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1186\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1187\u001b[0m )\n\u001b[1;32m   1189\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m   1191\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:933\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m    922\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    923\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    924\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m         position_ids,\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    934\u001b[0m         hidden_states,\n\u001b[1;32m    935\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    936\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    937\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    938\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    939\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    940\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    941\u001b[0m     )\n\u001b[1;32m    943\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:554\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache, position_ids)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_layer_norm_before:\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    555\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[1;32m    557\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = \"From 12:00 AM to 7:59 AM, these hours are available but less ideal for a doctor's appointment, resulting in a high cost of 200. Between 8:00 AM and 11:59 AM, work constraints render scheduling impossible, with an INFINITE cost. From 12:00 PM to 12:59 PM, this hour is optimal for a doctor's appointment, with a cost of 0. Between 1:00 PM and 4:59 PM, work hours make scheduling unavailable, resulting in an INFINITE cost. From 5:00 PM to 8:59 PM, this period is flexible and available, so the cost is 0. Finally, from 9:00 PM to 11:59 PM, late hours are less ideal for a doctor's appointment, resulting in a high cost of 500.\"\n",
    "gen = generate_cost_table(p)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from outlines import models, generate\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "\n",
    "class hour(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid', additionalProperties = False)\n",
    "    hour_range: str\n",
    "    cost: int\n",
    "\n",
    "class hourCost(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid', additionalProperties = False)\n",
    "    hour01: hour\n",
    "    hour02: hour\n",
    "    hour03: hour\n",
    "    hour04: hour\n",
    "    hour05: hour\n",
    "    hour06: hour\n",
    "    hour07: hour\n",
    "    hour08: hour\n",
    "    hour09: hour\n",
    "    hour10: hour\n",
    "    hour11: hour\n",
    "    hour12: hour\n",
    "    hour13: hour\n",
    "    hour14: hour\n",
    "    hour15: hour\n",
    "    hour16: hour\n",
    "    hour17: hour\n",
    "    hour18: hour\n",
    "    hour19: hour\n",
    "    hour20: hour\n",
    "    hour21: hour\n",
    "    hour22: hour\n",
    "    hour23: hour\n",
    "    hour24: hour\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ForcedGeneration:\n",
    "   \n",
    "    def __init__(self, model):\n",
    "        self.openai = models.openai(\"gpt-4o\", api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "        self.llm = models.transformers(model)    \n",
    "        self.generator = generate.json(self.llm, hourCost)\n",
    "        self.openaigenerator = generate.json(self.openai, hourCost)\n",
    "        self.completed_json = {\n",
    "            \"12:00AM-12:59AM\": -1,\n",
    "            \"1:00AM-1:59AM\": 0-1,\n",
    "            \"2:00AM-2:59AM\": -1,\n",
    "            \"3:00AM-3:59AM\": -1,\n",
    "            \"4:00AM-4:59AM\": -1,\n",
    "            \"5:00AM-5:59AM\": -1,\n",
    "            \"6:00AM-6:59AM\": -1,\n",
    "            \"7:00AM-7:59AM\": -1,\n",
    "            \"8:00AM-8:59AM\": -1,\n",
    "            \"9:00AM-9:59AM\": -1,\n",
    "            \"10:00AM-10:59AM\": -1,\n",
    "            \"11:00AM-11:59AM\": -1,\n",
    "            \"12:00PM-12:59PM\": -1,\n",
    "            \"1:00PM-1:59PM\": -1,\n",
    "            \"2:00PM-2:59PM\": -1,\n",
    "            \"3:00PM-3:59PM\": -1,\n",
    "            \"4:00PM-4:59PM\": -1,\n",
    "            \"5:00PM-5:59PM\": -1,\n",
    "            \"6:00PM-6:59PM\": -1,\n",
    "            \"7:00PM-7:59PM\": -1,\n",
    "            \"8:00PM-8:59PM\": -1,\n",
    "            \"9:00PM-9:59PM\": -1,\n",
    "            \"10:00PM-10:59PM\": -1,\n",
    "            \"11:00PM-11:59PM\": -1\n",
    "        }\n",
    "        print(\"generator loaded\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "            # self.target_token_ids = [self.tokenizer.encode(token) for token in self.completed_array]\n",
    "\n",
    "    def call(self, nl_context):\n",
    "        sentence_so_far = nl_context\n",
    "        # keys = list(self.completed_json.keys())\n",
    "\n",
    "        # for hour in keys:\n",
    "        #     # Add the current hour's description to the context\n",
    "        #     sentence_so_far += f\"Generate the json object for hour: {hour}: \"\n",
    "            \n",
    "        #     # Use the generator to compute the cost\n",
    "        #     try:\n",
    "        #         gen = self.generator(sentence_so_far, max_tokens=3)\n",
    "        #         print(f\"Generated (Transformer): {gen}\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"Error with Transformer generation: {e}\")\n",
    "        #         gen = \"500\"  # Fallback cost in case of error\n",
    "\n",
    "        #     try:\n",
    "        #         openai_gen = self.openaigenerator(sentence_so_far, max_tokens=3)\n",
    "        #         print(f\"Generated (OpenAI): {openai_gen}\")\n",
    "        #         cost = int(openai_gen.strip())\n",
    "        #     except ValueError:\n",
    "        #         print(f\"Invalid cost generated by OpenAI: {openai_gen}. Defaulting to Transformer cost.\")\n",
    "        #         cost = int(gen.strip())\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"Error with OpenAI generation: {e}. Defaulting to 500.\")\n",
    "        #         cost = 500  # Fallback cost in case of error\n",
    "\n",
    "        #     # Save the generated cost for the current hour\n",
    "        #     self.completed_json[hour] = cost\n",
    "\n",
    "        #     # Return the completed JSON as a string\n",
    "        #     return json.dumps(self.completed_json, indent=4)\n",
    "        openai_gen = None\n",
    "        i=0\n",
    "        while i < 5:\n",
    "            i += 1\n",
    "            try:\n",
    "                openai_gen = self.openaigenerator(sentence_so_far, max_tokens=10)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error with OpenAI generation: {e}\")\n",
    "                sentence_so_far += \"\\n Notice not to re-create the following error: \" + str(e)\n",
    "                print(i)\n",
    "                continue\n",
    "        \n",
    "        if openai_gen is None:\n",
    "            print(\"OpenAI generation failed. Defaulting to 500.\")\n",
    "            openai_gen = \"500\"\n",
    "            \n",
    "        print(f\"Generated (OpenAI): {openai_gen}\")\n",
    "\n",
    "        gen = self.generator(sentence_so_far, max_tokens=10)\n",
    "        print(f\"Generated (Transformer): {gen}\")\n",
    "        \n",
    "        return gen\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    prompt = \"From 12:00 AM to 7:59 AM, these hours are available but less ideal for a doctor's appointment, resulting in a high cost of 200. Between 8:00 AM and 11:59 AM, work constraints render scheduling impossible, with an INFINITE cost. From 12:00 PM to 12:59 PM, this hour is optimal for a doctor's appointment, with a cost of 0. Between 1:00 PM and 4:59 PM, work hours make scheduling unavailable, resulting in an INFINITE cost. From 5:00 PM to 8:59 PM, this period is flexible and available, so the cost is 0. Finally, from 9:00 PM to 11:59 PM, late hours are less ideal for a doctor's appointment, resulting in a high cost of 500.\"\n",
    "    context = f\"\"\"\n",
    "            Given the following schema definitions:\n",
    "\n",
    "            class hour(BaseModel):\n",
    "                hour_range: str\n",
    "                cost: int\n",
    "\n",
    "            class hourCost(BaseModel):\n",
    "                model_config = ConfigDict(extra='forbid')\n",
    "                hour01: hour\n",
    "                hour02: hour\n",
    "                hour03: hour\n",
    "                hour04: hour\n",
    "                hour05: hour\n",
    "                hour06: hour\n",
    "                hour07: hour\n",
    "                hour08: hour\n",
    "                hour09: hour\n",
    "                hour10: hour\n",
    "                hour11: hour\n",
    "                hour12: hour\n",
    "                hour13: hour\n",
    "                hour14: hour\n",
    "                hour15: hour\n",
    "                hour16: hour\n",
    "                hour17: hour\n",
    "                hour18: hour\n",
    "                hour19: hour\n",
    "                hour20: hour\n",
    "                hour21: hour\n",
    "                hour22: hour\n",
    "                hour23: hour\n",
    "                hour24: hour\n",
    "\n",
    "            ### Example 1:\n",
    "            **Description:** From 12:00 AM to 6:59 AM, scheduling is unreasonable due to a preference against early appointments, so this time has a very high cost of 1000. From 7:00 AM to 8:59 AM, the time is available but less ideal, warranting a moderate cost of 50. From 9:00 AM to 2:00 PM, work constraints render these times unavailable with an INFINITE cost. Between 2:00 PM and 2:59 PM, this period is flexible and ideal for the doctor's appointment, so it has a cost of 0. From 3:00 PM to 3:59 PM, the dentist appointment makes this hour unavailable with an INFINITE cost. From 4:00 PM to 9:00 PM, scheduling is optimal, also with a cost of 0. Finally, from 9:01 PM to 11:59 PM, late hours are less preferable and have a high cost of 500.\n",
    "\n",
    "            **Output JSON:**\n",
    "            {{\n",
    "            \"hour01\": {{\"hour_range\": \"12:00 AM - 12:59 AM\", \"cost\": 1000}},\n",
    "            \"hour02\": {{\"hour_range\": \"1:00 AM - 1:59 AM\", \"cost\": 1000}},\n",
    "            \"hour03\": {{\"hour_range\": \"2:00 AM - 2:59 AM\", \"cost\": 1000}},\n",
    "            \"hour04\": {{\"hour_range\": \"3:00 AM - 3:59 AM\", \"cost\": 1000}},\n",
    "            \"hour05\": {{\"hour_range\": \"4:00 AM - 4:59 AM\", \"cost\": 1000}},\n",
    "            \"hour06\": {{\"hour_range\": \"5:00 AM - 5:59 AM\", \"cost\": 1000}},\n",
    "            \"hour07\": {{\"hour_range\": \"6:00 AM - 6:59 AM\", \"cost\": 1000}},\n",
    "            \"hour08\": {{\"hour_range\": \"7:00 AM - 7:59 AM\", \"cost\": 50}},\n",
    "            \"hour09\": {{\"hour_range\": \"8:00 AM - 8:59 AM\", \"cost\": 50}},\n",
    "            \"hour10\": {{\"hour_range\": \"9:00 AM - 9:59 AM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour11\": {{\"hour_range\": \"10:00 AM - 10:59 AM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour12\": {{\"hour_range\": \"11:00 AM - 11:59 AM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour13\": {{\"hour_range\": \"12:00 PM - 12:59 PM\", \"cost\": 0}},\n",
    "            \"hour14\": {{\"hour_range\": \"1:00 PM - 1:59 PM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour15\": {{\"hour_range\": \"2:00 PM - 2:59 PM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour16\": {{\"hour_range\": \"3:00 PM - 3:59 PM\", \"cost\": \"INFINITE\"}},\n",
    "            \"hour17\": {{\"hour_range\": \"4:00 PM - 4:59 PM\", \"cost\": 0}},\n",
    "            \"hour18\": {{\"hour_range\": \"5:00 PM - 5:59 PM\", \"cost\": 0}},\n",
    "            \"hour19\": {{\"hour_range\": \"6:00 PM - 6:59 PM\", \"cost\": 0}},\n",
    "            \"hour20\": {{\"hour_range\": \"7:00 PM - 7:59 PM\", \"cost\": 0}},\n",
    "            \"hour21\": {{\"hour_range\": \"8:00 PM - 8:59 PM\", \"cost\": 0}},\n",
    "            \"hour22\": {{\"hour_range\": \"9:00 PM - 9:59 PM\", \"cost\": 500}},\n",
    "            \"hour23\": {{\"hour_range\": \"10:00 PM - 10:59 PM\", \"cost\": 500}},\n",
    "            \"hour24\": {{\"hour_range\": \"11:00 PM - 11:59 PM\", \"cost\": 500}}\n",
    "            }}\n",
    "\n",
    "            \n",
    "            Convert the following description into a JSON object matching the schema:\n",
    "\n",
    "            {prompt}\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "                \n",
    "    model = \"facebook/bart-large\"\n",
    "    # model = \"facebook/opt-iml-max-1.3b\"\n",
    "    gen = ForcedGeneration(model)\n",
    "    result = gen.call(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loaded\n",
      "Error with OpenAI generation: 1 validation error for hourCost\n",
      "__root__\n",
      "  Unterminated string starting at: line 1 column 25 (char 24) [type=value_error.jsondecode, input_value='{\"hour01\":{\"hour_range\":\"12:00', input_type=str]\n",
      "1\n",
      "Error with OpenAI generation: 1 validation error for hourCost\n",
      "__root__\n",
      "  Unterminated string starting at: line 1 column 25 (char 24) [type=value_error.jsondecode, input_value='{\"hour01\":{\"hour_range\":\"12:00', input_type=str]\n",
      "2\n",
      "Error with OpenAI generation: 1 validation error for hourCost\n",
      "__root__\n",
      "  Unterminated string starting at: line 1 column 25 (char 24) [type=value_error.jsondecode, input_value='{\"hour01\":{\"hour_range\":\"12:00', input_type=str]\n",
      "3\n",
      "Error with OpenAI generation: 1 validation error for hourCost\n",
      "__root__\n",
      "  Unterminated string starting at: line 1 column 25 (char 24) [type=value_error.jsondecode, input_value='{\"hour01\":{\"hour_range\":\"12:00', input_type=str]\n",
      "4\n",
      "Error with OpenAI generation: 1 validation error for hourCost\n",
      "__root__\n",
      "  Unterminated string starting at: line 1 column 25 (char 24) [type=value_error.jsondecode, input_value='{\"hour01\":{\"hour_range\":\"12:00', input_type=str]\n",
      "5\n",
      "OpenAI generation failed. Defaulting to 500.\n",
      "Generated (OpenAI): 500\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for hourCost\n__root__\n  Expecting value: line 1 column 11 (char 10) [type=value_error.jsondecode, input_value='{\"hour01\":', input_type=str]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\pydantic\\main.py:1249\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[1;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1249\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_str_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\pydantic\\deprecated\\parse.py:49\u001b[0m, in \u001b[0;36mload_str_bytes\u001b[1;34m(b, content_type, encoding, proto, allow_pickle, json_loads)\u001b[0m\n\u001b[0;32m     48\u001b[0m         b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m proto \u001b[38;5;241m==\u001b[39m Protocol\u001b[38;5;241m.\u001b[39mpickle:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 11 (char 10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 79\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# model = \"facebook/opt-iml-max-1.3b\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m gen \u001b[38;5;241m=\u001b[39m ForcedGeneration(model)\n\u001b[1;32m---> 79\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[3], line 133\u001b[0m, in \u001b[0;36mForcedGeneration.call\u001b[1;34m(self, nl_context)\u001b[0m\n\u001b[0;32m    129\u001b[0m     openai_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated (OpenAI): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenai_gen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_so_far\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated (Transformer): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen\n",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\outlines\\generate\\api.py:512\u001b[0m, in \u001b[0;36mSequenceGeneratorAdapter.__call__\u001b[1;34m(self, prompts, max_tokens, stop_at, seed, **model_specific_params)\u001b[0m\n\u001b[0;32m    500\u001b[0m generation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_generation_parameters(\n\u001b[0;32m    501\u001b[0m     max_tokens, stop_at, seed\n\u001b[0;32m    502\u001b[0m )\n\u001b[0;32m    504\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    505\u001b[0m     prompts,\n\u001b[0;32m    506\u001b[0m     generation_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_specific_params,\n\u001b[0;32m    510\u001b[0m )\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\outlines\\generate\\api.py:488\u001b[0m, in \u001b[0;36mSequenceGeneratorAdapter._format\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format(sequence) \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m sequences]\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\outlines\\generate\\json.py:55\u001b[0m, in \u001b[0;36mjson.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     53\u001b[0m     regex_str \u001b[38;5;241m=\u001b[39m build_regex_from_schema(schema, whitespace_pattern)\n\u001b[0;32m     54\u001b[0m     generator \u001b[38;5;241m=\u001b[39m regex(model, regex_str, sampler)\n\u001b[1;32m---> 55\u001b[0m     generator\u001b[38;5;241m.\u001b[39mformat_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mschema_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema_object, \u001b[38;5;28mtype\u001b[39m(Enum)):\n\u001b[0;32m     57\u001b[0m     schema \u001b[38;5;241m=\u001b[39m pyjson\u001b[38;5;241m.\u001b[39mdumps(get_schema_from_enum(schema_object))\n",
      "File \u001b[1;32mc:\\Users\\Dorian\\Documents\\RBR\\venv\\Lib\\site-packages\\pydantic\\main.py:1276\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[1;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     error: pydantic_core\u001b[38;5;241m.\u001b[39mInitErrorDetails \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;66;03m# The type: ignore on the next line is to ignore the requirement of LiteralString\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: pydantic_core\u001b[38;5;241m.\u001b[39mPydanticCustomError(type_str, \u001b[38;5;28mstr\u001b[39m(exc)),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m'\u001b[39m,),\n\u001b[0;32m   1274\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: b,\n\u001b[0;32m   1275\u001b[0m     }\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pydantic_core\u001b[38;5;241m.\u001b[39mValidationError\u001b[38;5;241m.\u001b[39mfrom_exception_data(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, [error])\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_validate(obj)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for hourCost\n__root__\n  Expecting value: line 1 column 11 (char 10) [type=value_error.jsondecode, input_value='{\"hour01\":', input_type=str]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Valid: True\n",
      "\n",
      "Validating: [0.5, -1.2, float('inf'), 3.14, -0.01, 100, 0, -99.99, float('inf'), 42, 0.001, -0.1, 1000, -1000, 0.5, -0.5, 3.14159, -3.14159, 2.718, -2.718, 1.414, -1.414, 0.707, -0.707]\n",
      "Valid: True\n",
      "\n",
      "Validating: [-1, 2, float('inf'), 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Valid: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern to match an array of 24 numbers\n",
    "number_array_pattern = r'^\\s*\\[\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+))),\\s*(-?(?:\\d+\\.?\\d*|float\\(\\'inf\\'\\)|-?(?:\\.\\d+)))\\s*\\]$'\n",
    "\n",
    "# Function to validate the array\n",
    "def validate_number_array(array_string):\n",
    "    return re.match(number_array_pattern, array_string) is not None\n",
    "\n",
    "# Example usage\n",
    "examples = [\n",
    "    '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]',\n",
    "    '[0.5, -1.2, float(\\'inf\\'), 3.14, -0.01, 100, 0, -99.99, float(\\'inf\\'), 42, 0.001, -0.1, 1000, -1000, 0.5, -0.5, 3.14159, -3.14159, 2.718, -2.718, 1.414, -1.414, 0.707, -0.707]',\n",
    "    '[-1, 2, float(\\'inf\\'), 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]'\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(f\"Validating: {example}\")\n",
    "    print(f\"Valid: {validate_number_array(example)}\\n\")"
   ]
=======
>>>>>>> parent of 0049021 (single agnet llm - outlines and pydantic attempt)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
